{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ir.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "cfc4dac7468c084d307aa08ac9296834a0ec3041907b428597ed5d5794ddbf16"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_env(env_path):\n",
        "    with open(env_path, 'r', encoding=\"utf-8\") as env:\n",
        "            env_settings = env.read()\n",
        "            env_settings = re.sub(' ','',env_settings)\n",
        "            env_settings = env_settings.split('\\n')\n",
        "            env_settings = [setting.split('=') for setting in env_settings]\n",
        "            return dict(env_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def settings(env_settings):\n",
        "    language = env_settings['language']\n",
        "    path = env_settings['path']\n",
        "    type_files = env_settings['type_files']\n",
        "    number_files= int(env_settings['number_files'])\n",
        "    return language,path,type_files,number_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "language,path,type_files,number_files=settings(read_env('env.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fileNames(path,type_files):\n",
        "    file_names = [name for name in os.listdir(path) if name.endswith(type_files)]\n",
        "    file_dirs=[]\n",
        "    for name in file_names:\n",
        "        file_dirs.append(path + '/' +name)\n",
        "    return file_dirs,file_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Stemming(tokins):\n",
        "    stemmer = PorterStemmer() \n",
        "    reviews_stem = [] \n",
        "    reviews_stem = [stemmer.stem(word) for word in tokins]\n",
        "    return reviews_stem "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(query): # get text and return list of tokens withot stopwords and lowercase \n",
        "    stopword = stopwords.words(language) #get list of stop words in language\n",
        "    tokens = word_tokenize(query,language=language)  #get list of tokens\n",
        "    tokens_without_stop_word = [word.lower() for word in tokens if not word in stopword] # remove stopwords\n",
        "    return  tokens_without_stop_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tf_dict():\n",
        "    doc_vocab  = dict()\n",
        "    file_dirs,file_names =fileNames(path,type_files)\n",
        "    for i in range(0,len(file_dirs)):\n",
        "        doc_vocab[i] = dict()\n",
        "        with open(file_dirs[i], 'r', encoding=\"utf-8\") as doc:\n",
        "            read_string = doc.read()  \n",
        "            tokens = tokenize(read_string)\n",
        "            # tokens = Stemming(tokens)\n",
        "            # get dict of {doc_num : {word1 : word1_count, word2 : word2_count, .... }}\n",
        "            for words in tokens:\n",
        "                if words in doc_vocab[i]:\n",
        "                    doc_vocab[i][words] += 1\n",
        "                else:\n",
        "                    doc_vocab[i][words] = 1\n",
        "\n",
        "    term_pd = pd.DataFrame.from_dict(doc_vocab, orient='index')\n",
        "    term_pd.fillna(0,inplace=True)\n",
        "    term_pd.sort_index(inplace=True)\n",
        "    term_pd.to_csv('tf.csv')\n",
        "    return term_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "term_tf=tf_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ls=term_tf[term_tf > 0].count()\n",
        "idf=np.log(number_files/df_ls.values)\n",
        "idf_dict=pd.Series(data=idf,index=df_ls.index).to_dict()\n",
        "tf_idf=term_tf.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "for term in term_tf:\n",
        "    for doc in range(0,len(term_tf)):\n",
        "        if term_tf[term][doc] == 0:\n",
        "            tf_idf[term][doc] = 0\n",
        "        else:\n",
        "            tf_idf[term][doc] = (1 + np.log(term_tf[term][doc])) * np.log10(number_files/df_ls[term])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_idf.to_csv('tf_idf2.csv')\n"
      ]
    }
  ]
}