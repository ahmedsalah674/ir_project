{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- imports---------\n",
    "import nltk #to import tokenize            \n",
    "from nltk.corpus import stopwords #to remove stopwords\n",
    "from nltk.tokenize import word_tokenize #tokenize text and lowercacse it\n",
    "import os\n",
    "from os import path # to check if the file is there or not \n",
    "from nltk.stem import PorterStemmer # to stem tokiens\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_env(env_path):\n",
    "    with open(env_path, 'r', encoding=\"utf-8\") as env:\n",
    "        env_settings = env.read()\n",
    "        env_settings = re.sub(' ','',env_settings)\n",
    "        env_settings = env_settings.split('\\n')\n",
    "        env_settings = [setting.split('=') for setting in env_settings]\n",
    "    return dict(env_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def settings(env_settings):\n",
    "    language = env_settings['language']\n",
    "    path = env_settings['path']\n",
    "    type_files = env_settings['type_files']\n",
    "    number_files= int(env_settings['number_files'])\n",
    "    download= bool(int(env_settings['download']))\n",
    "    return language,path,type_files,number_files,download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "language,path,type_files,number_files,download=settings(read_env('env.txt'))\n",
    "#-----------downlad------------\n",
    "if download:\n",
    "    nltk.download(['stopwords','punkt']) # if you don't have packages run it for one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_type_model():\n",
    "    type_query=input('please enter 1 for Positional index model or 2 for Vector space model or 0 for exit: ')\n",
    "    if (type_query=='1'):\n",
    "        Positional_index_model()\n",
    "    elif(type_query=='2'):\n",
    "        # Positional_index_model()\n",
    "        print('vector space model')\n",
    "    elif(type_query=='0'):\n",
    "        print('thanks :)')\n",
    "        time.sleep(3)\n",
    "    else:\n",
    "        print('\\t\\tsorry put you entered wrong number try again')\n",
    "        scan_type_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ls_steming_tokins():\n",
    "    inquery = input(\"Please enter a query:\\n\")\n",
    "    query_to = tokenize(inquery)\n",
    "    query =Stemming(query_to)\n",
    "    return query,query_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming(tokins):\n",
    "    stemmer = PorterStemmer() \n",
    "    reviews_stem = [] \n",
    "    reviews_stem = [stemmer.stem(word) for word in tokins]\n",
    "    return reviews_stem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(query): # get text and return list of tokens withot stopwords and lowercase \n",
    "    stopword = stopwords.words(language) #get list of stop words in language\n",
    "    tokens = word_tokenize(query,language=language)  #get list of tokens\n",
    "    tokens_without_stop_word = [word.lower() for word in tokens if not word in stopword] # remove stopwords\n",
    "    return  tokens_without_stop_word"
   ]
  },
  {
   "source": [
    "def positional (tokens): # get tokens and return\n",
    "    pos_index = dict() \n",
    "    for index, term in enumerate(tokens): # index is int value for index and term is str value for token\n",
    "        if term in pos_index:\n",
    "            pos_index[term].append(index + 1)\n",
    "        else:\n",
    "            pos_index[term] = []\n",
    "            pos_index[term].append(index + 1)\n",
    "    return pos_index"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docID(plist): # get list and return number of doc in index 0 plist =[docID,[indices]]\n",
    "    return plist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position(plist): # get list and return number of doc in index 0 plist =[docID,[indices]]\n",
    "    return plist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_intersect(p1, p2, k): \n",
    "    answer = []\n",
    "    len1 = len(p1)\n",
    "    len2 = len(p2)\n",
    "    i = j = 0\n",
    "    while i != len1 and j != len2:  # p1 != null and p2 != null\n",
    "        if docID(p1[i]) == docID(p2[j]):\n",
    "            l = []  # l <- ()\n",
    "            pp1 = position(p1[i])  # pp1 <- positions(p1) , p1[i]=[docID,[indices]]\n",
    "            pp2 = position(p2[j])  # pp2 <- positions(p2) , p1[i]=[docID,[indices]]\n",
    "            plen1 = len(pp1)\n",
    "            plen2 = len(pp2)\n",
    "            ii = jj = 0\n",
    "            while ii != plen1:  # while (pp1 != null)\n",
    "                while jj != plen2:  # while (pp2 != null)\n",
    "                    if abs(pp1[ii] - pp2[jj]) == k:  # if (|index(pp1) - index(pp2)| == k) , k=1\n",
    "                        l.append(pp2[jj]) \n",
    "                    elif pp2[jj] > pp1[ii]:  # index(pp2) > index(pp1)\n",
    "                        break\n",
    "                    jj += 1  # next(pp2)->int other index for second term\n",
    "                answer.append([docID(p1[i]),  l])  #answer=[docID, index(first_term), ps_secondTerm]\n",
    "                ii += 1  #  next(pp1)->int other index for first term\n",
    "            i += 1  #  next(p1) ,p1=[docID,[indices]]\n",
    "            j += 1  #  next(p2) ,p2=[docID,[indices]]\n",
    "        elif docID(p1[i]) < docID(p2[j]):  # increace counter of term if his docID is smaller than the other\n",
    "            i += 1  #  next(p1)\n",
    "        else:\n",
    "            j += 1  #  next(p2)\n",
    "    return answer"
   ]
  },
  {
   "source": [
    "def fileNames(path,type_str):\n",
    "    file_names = [name for name in os.listdir(path) if name.endswith(type_str)]\n",
    "    file_dirs=[]\n",
    "    for name in file_names:\n",
    "        file_dirs.append(path + '/' +name)\n",
    "    return file_dirs,file_names"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Positional_index_model():\n",
    "    positions = dict()\n",
    "    files=list()\n",
    "    finalresult = list()\n",
    "    query,query_to=get_ls_steming_tokins()\n",
    "    # print('\\n',query,'\\n')\n",
    "    for term in query:\n",
    "        positions[term]=list()\n",
    "        positions[term].append(0)\n",
    "        positions[term].append([])\n",
    "    file_dirs,file_names =fileNames(path,type_files)\n",
    "    for i in range(0,len(file_dirs)):\n",
    "        with open(file_dirs[i], 'r', encoding=\"utf-8\") as doc:\n",
    "            read_string = doc.read()\n",
    "            tokens=tokenize(read_string)\n",
    "            tokens=Stemming(tokens)\n",
    "            pos_index=positional(tokens)\n",
    "            for word in query:\n",
    "                for pos, term in enumerate(pos_index):\n",
    "                    if term == word:\n",
    "                        positions[word][1].append([i, pos_index[word]])\n",
    "                        positions[word][0]=positions[word][0]+1\n",
    "    # print(positions)\n",
    "    ra=0\n",
    "    for term in positions:\n",
    "        print(query_to[ra],' :',positions[term]) #dict {term: [term_freq,[[doc1,[indices]],[doc2,[indices]]]]}\n",
    "        ra +=1\n",
    "    # print('len(q)->',len(query)) \n",
    "    if len(query)!=1:\n",
    "\n",
    "        i=0\n",
    "        j=0\n",
    "        while i < len(query):\n",
    "            j=i+1\n",
    "            if j<len(query):\n",
    "                # send positions without freq positions[query][1]->[[doc1,[inindices],[doc2,[inindices],...]]\n",
    "                files.append(pos_intersect(positions[query[i]][1], positions[query[j]][1], 1)) #get intersect of two terms in query\n",
    "            i=i+1\n",
    "        b = 1\n",
    "        lfiles =len(files)  #files=[[[doc1,index],[doc2,index]],[[doc1,index],[doc2,index]],....]\n",
    "        #  -###################     call pos_intersect on the output of the last loop \n",
    "        while b<lfiles: #loop N-1 time on intersect of lists \n",
    "            i=0\n",
    "            j=0\n",
    "            while i < len(files):#files=[[[doc1,index],[doc2,index]],[[doc1,index],[doc2,index]],....]\n",
    "                j=i+1\n",
    "                if j<len(files):\n",
    "                    finalresult.append(pos_intersect(files[i], files[j], 1))\n",
    "                i=i+1\n",
    "            files=finalresult.copy()\n",
    "            finalresult=list()\n",
    "            b +=1\n",
    "        #######################\n",
    "        if files!=[]:\n",
    "            docs=set([i[0] for fi in files for i in fi if i[1]!=[]])\n",
    "            for i in docs:print(file_names[i])\n",
    "            scan_type_model()\n",
    "        else :\n",
    "            print('this query dosen\\'t match ')\n",
    "            scan_type_model()\n",
    "    else:\n",
    "        # for i in positions[query[0]][1]:\n",
    "            # print(i)\n",
    "        files=[docID(i) for i in positions[query[0]][1]]\n",
    "        if files !=[]:\n",
    "            for i in set(files):print(file_names[i])\n",
    "            scan_type_model()\n",
    "        else :\n",
    "            print('this query dosen\\'t match ')\n",
    "            scan_type_model()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ahmed  : [7, [[0, [1, 17]], [1, [1]], [2, [19]], [6, [1]], [7, [19]], [8, [1]], [9, [19]]]]\n",
      "aly  : [8, [[0, [6]], [1, [5]], [2, [20]], [3, [5]], [6, [6]], [7, [20]], [8, [6]], [9, [20]]]]\n",
      "andrew  : [7, [[0, [5]], [1, [16]], [2, [21]], [3, [6]], [6, [5]], [7, [21]], [8, [5]]]]\n",
      "doc2.txt\n",
      "doc7.txt\n",
      "thanks :)\n"
     ]
    }
   ],
   "source": [
    "scan_type_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cfc4dac7468c084d307aa08ac9296834a0ec3041907b428597ed5d5794ddbf16"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}