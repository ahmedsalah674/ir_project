{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- imports---------\n",
    "import nltk #to import tokenize            \n",
    "from nltk.corpus import stopwords #to remove stopwords\n",
    "from nltk.tokenize import word_tokenize #tokenize text and lowercacse it\n",
    "import os\n",
    "from os import path # to check if the file is there or not \n",
    "from nltk.stem import PorterStemmer # to stem tokiens\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------downlad------------\n",
    "# nltk.download(['stopwords','punkt']) # if you don't have packages run it for one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "language='english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_type_model():\n",
    "    type_query=input('please enter 1 for Positional index model or 2 for Vector space model or 0 for exit: ')\n",
    "    if (type_query=='1'):\n",
    "        Positional_index_model()\n",
    "    elif(type_query=='2'):\n",
    "        Positional_index_model()\n",
    "    elif(type_query=='0'):\n",
    "        print('thanks :)')\n",
    "        time.sleep(3)\n",
    "    else:\n",
    "        print('\\t\\tsorry put you entered wrong number try again')\n",
    "        scan_type_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming(tokins):\n",
    "    stemmer = PorterStemmer() \n",
    "    reviews_stem = [] \n",
    "    reviews_stem = [stemmer.stem(word) for word in tokins]\n",
    "    return reviews_stem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(query): # get text and return list of tokens withot stopwords and lowercase \n",
    "    stopword = stopwords.words(language) #get list of stop words in language\n",
    "    tokens = word_tokenize(query,language=language)  #get list of tokens\n",
    "    tokens_without_stop_word = [word.lower() for word in tokens if not word in stopword] # remove stopwords\n",
    "    return  tokens_without_stop_word"
   ]
  },
  {
   "source": [
    "def positional (tokens): # get tokens and return\n",
    "    pos_index = dict() \n",
    "    for index, term in enumerate(tokens): # index is int value for index and term is str value for token\n",
    "        if term in pos_index:\n",
    "            pos_index[term].append(index + 1)\n",
    "        else:\n",
    "            pos_index[term] = []\n",
    "            pos_index[term].append(index + 1)\n",
    "    return pos_index"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docID(plist): # get list and return number of doc in index 0 plist =[docID,[indices]]\n",
    "    return plist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position(plist): # get list and return number of doc in index 0 plist =[docID,[indices]]\n",
    "    return plist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_intersect(p1, p2, k):\n",
    "    answer = []\n",
    "    len1 = len(p1)\n",
    "    len2 = len(p2)\n",
    "    i = j = 0\n",
    "    while i != len1 and j != len2:  # p1 != null and p2 != null\n",
    "        if docID(p1[i]) == docID(p2[j]):\n",
    "            l = []  # l <- ()\n",
    "            pp1 = position(p1[i])  # pp1 <- positions(p1) , p1[i]=[docID,[indices]]\n",
    "            pp2 = position(p2[j])  # pp2 <- positions(p2) , p1[i]=[docID,[indices]]\n",
    "            # print(pp1,'\\t',pp2)\n",
    "            plen1 = len(pp1)\n",
    "            plen2 = len(pp2)\n",
    "            ii = jj = 0\n",
    "            while ii != plen1:  # while (pp1 != null)\n",
    "                while jj != plen2:  # while (pp2 != null)\n",
    "                    if abs(pp1[ii] - pp2[jj]) == k:  # if (|index(pp1) - index(pp2)| == k) , k=1\n",
    "                        l.append(pp2[jj]) \n",
    "                    elif pp2[jj] > pp1[ii]:  # index(pp2) > index(pp1)\n",
    "                        break\n",
    "                    jj += 1  # next(pp2)->int other index for second term\n",
    "                # while l != [] and abs(l[0] - pp1[ii]) > k:  # while (l != () and |l(0) - pos(pp1)| > k)\n",
    "                #     l.remove(l[0])  # delete(l[0])\n",
    "                # print (l)\n",
    "                for ps in l:  #indices\n",
    "                    answer.append([docID(p1[i]), pp1[ii], ps])  #answer=[docID, index(first_term), ps_secondTerm]\n",
    "                ii += 1  #  next(pp1)->int other index for first term\n",
    "            i += 1  #  next(p1) ,p1=[docID,[indices]]\n",
    "            j += 1  #  next(p2) ,p2=[docID,[indices]]\n",
    "        elif docID(p1[i]) < docID(p2[j]):  # increace counter of term if his docID is smaller than the other\n",
    "            i += 1  #  next(p1)\n",
    "        else:\n",
    "            j += 1  #  next(p2)\n",
    "    return answer"
   ]
  },
  {
   "source": [
    "def fileNames(path,type_str):\n",
    "    file_names = [name for name in os.listdir(path) if name.endswith(type_str)]\n",
    "    file_dirs=[]\n",
    "    for name in file_names:\n",
    "        file_dirs.append(path + '/' +name)\n",
    "    return file_dirs,file_names"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Positional_index_model():\n",
    "    positions = dict()\n",
    "    inquery = input(\"Please enter a query:\\n\")\n",
    "    query = tokenize(inquery)\n",
    "    query =Stemming(query)\n",
    "    for term in query:\n",
    "        positions[term]=list()\n",
    "        positions[term].append(0)\n",
    "        positions[term].append([])\n",
    "    # for i in range(1, 4):\n",
    "    #     docName='files\\\\doc%d.txt'%i\n",
    "    file_dirs,file_names =fileNames('files','txt')\n",
    "    for i in range(0,len(file_dirs)):\n",
    "        with open(file_dirs[i], 'r', encoding=\"utf-8\") as doc:\n",
    "            read_string = doc.read()\n",
    "            tokens=tokenize(read_string)\n",
    "            tokens=Stemming(tokens)\n",
    "            pos_index=positional(tokens)\n",
    "            for word in query:\n",
    "                for pos, term in enumerate(pos_index):\n",
    "                    if term == word:\n",
    "                        positions[word][1].append([i, pos_index[word]])\n",
    "                        positions[word][0]=positions[word][0]+1\n",
    "\n",
    "    print(positions) #dict {term: [term_freq,[[doc1,[indices]],[doc2,[indices]]]]}\n",
    "\n",
    "    i=0\n",
    "    j=0\n",
    "    files=list()\n",
    "    finalresult = list()\n",
    "    while i < len(query):\n",
    "        j=i+1\n",
    "        if j<len(query):\n",
    "            files.append(pos_intersect(positions[query[i]][1], positions[query[j]][1], 1))# send positions without freq\n",
    "        i=j+1\n",
    "\n",
    "    for fi in files:\n",
    "        for i in fi:\n",
    "            finalresult.append(i[0])\n",
    "    if finalresult !=[]:\n",
    "        print('the docs that match your query is:')\n",
    "        for i in set(finalresult):print(file_names[i])\n",
    "        scan_type_model()\n",
    "    else :\n",
    "        print('this query dosen\\'t match ')\n",
    "        scan_type_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'ahm': [5, [[0, [1]], [1, [19]], [2, [5]], [3, [1]], [4, [19]]]], 'moham': [2, [[0, [2]], [3, [2]]]]}\n",
      "the docs that match your query is:\n",
      "doc1.txt\n",
      "one.txt\n",
      "thanks :)\n"
     ]
    }
   ],
   "source": [
    "scan_type_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cfc4dac7468c084d307aa08ac9296834a0ec3041907b428597ed5d5794ddbf16"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}