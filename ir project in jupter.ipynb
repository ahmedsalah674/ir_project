{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- imports---------\n",
    "import nltk #to import tokenize            \n",
    "import re   #to remove spicieal letters \n",
    "from nltk.corpus import stopwords #to remove stopwords\n",
    "from nltk.tokenize import TweetTokenizer #tokenize text and lowercacse it\n",
    "from os import path # to check if the file is there or not \n",
    "from collections import Counter #to return counter in dic {word:count}\n",
    "from nltk.stem import PorterStemmer \n",
    "import os\n",
    "# from nltk.tokenize import word_tokenize  \n",
    "# import string \n",
    "#    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DR.LAP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer fun to return list of tokens in lowercase\n",
    "def rtokenize(text2):   \n",
    "    tokenizer = TweetTokenizer(preserve_case=True, strip_handles=True,reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(text2)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter text \n",
      "understanable have a great day\n",
      "['understanable', 'have', 'a', 'great', 'day']\n"
     ]
    }
   ],
   "source": [
    "#test tokenizer fun\n",
    "# me_text=input(\"enter text \\n\")\n",
    "me_text_tokin1=rtokenize(doc_lines1[0])\n",
    "me_text_tokin2=rtokenize(doc_lines2[0])\n",
    "# print(me_text_tokins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return set of words (terms)\n",
    "def set_tokins(tokins):\n",
    "    terms1=[]\n",
    "    for x in tokins:\n",
    "        if x not in terms1:\n",
    "            terms1.append(x)     \n",
    "    return terms1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set_tokins()\n",
    "set1=set_tokins(me_text_tokin1)\n",
    "set2=set_tokins(me_text_tokin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return counter for all items in word_1 list\n",
    "def get_count(word_l):\n",
    "    word_count_dict = {} \n",
    "    word_count_dict = Counter(word_l) \n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file in lines and return it in list line by line and if file doesn't exists return false \n",
    "# if split == false will return lines without any split \n",
    "# if split equal '\\t' or '\\n' will split lines as w need\n",
    "def read_from_file(doc_name,lines=True,split=False,both=False):\n",
    "        if isinstance(lines, bool) and isinstance(both ,bool):\n",
    "            if both and split:\n",
    "                return False \n",
    "            if path.exists(str(doc_name)):\n",
    "                file=open(str(doc_name),\"r\",encoding =\"ascii\")\n",
    "                if not lines:\n",
    "                    doc_lines=[file.readline()]\n",
    "                    if split and (split == '\\t' or split == '\\n'):\n",
    "                        doc_lines=[word for word in doc_lines[0].split(split) if word !='']\n",
    "                    elif split:\n",
    "                        return False\n",
    "                else :\n",
    "                    doc_lines=file.readlines()\n",
    "                    if split and (split == '\\t' or split == '\\n'):\n",
    "                        doc_lines=[word for line in doc_lines for word in line.split(split) if word !='']\n",
    "                    elif split:\n",
    "                        return False\n",
    "                file.close()\n",
    "                if both:\n",
    "                   doc_lines=[word.split('\\t') for line in doc_lines for word in line.split('\\n') if word !='' ]  \n",
    "                return doc_lines\n",
    "            else: \n",
    "                return False;\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['playing1', 'boys', 'girles'],\n",
       " ['playing2', 'boys', 'girles'],\n",
       " ['playing3', 'boys', 'girles'],\n",
       " ['playing4', 'boys', 'girles'],\n",
       " ['playing5', 'boys', 'girles'],\n",
       " ['playing6', 'boys', 'girles']]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name1='new file test.txt'\n",
    "lis=read_from_file(doc_name=file_name1,lines=True,split=False,both=True)\n",
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return indexes of one element \n",
    "def indexes(lis,val):\n",
    "    return[index for index, value in enumerate(lis) if value == val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 11]\n"
     ]
    }
   ],
   "source": [
    "# test indexes fun (list ,value) and return the indexes of value \n",
    "lis=[1,2,5,2,3,86,37,3781,72,1,1,2]\n",
    "print(indexes(lis,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming fun (list_of_words) and return another list after stem it \n",
    "def Stemming(tokins):\n",
    "    stemmer = PorterStemmer() \n",
    "    reviews_stem = [] \n",
    "    reviews_stem = [stemmer.stem(word) for word in tokins]\n",
    "    return reviews_stem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['playing1', 'boy', 'girl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test stemming fun (list_of_tokins)\n",
    "Stemming(lis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write func in file (file_name,list_contant,replase,Taps=False,Lines=False) return file have the name of file_name var and the contant list and if Taps = True return words in same line with \\t or Lines have  \n",
    "def write_file(file_name,lis,replace=False,Taps=False,Lines=False):\n",
    "    if replace:\n",
    "        file= open(file_name,\"w\")\n",
    "    else:\n",
    "        file= open(file_name,\"a\")\n",
    "    if Taps and not Lines :\n",
    "        file.write('\\t'.join(lis[0:]))\n",
    "        file.close()\n",
    "    elif Lines and not Taps :\n",
    "        file.write('\\n'.join(lis[0:]))\n",
    "        file.close()\n",
    "    else: \n",
    "        file.close()\n",
    "        return False\n",
    "    # else:\n",
    "    #     file= open(file_name,\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(lis=me_text_tokin1,file_name='new file test.txt',Taps=True,Lines=False,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    tokens_list = [word for word in tokens if not word in stopwords.words()]\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_index(tokens_list, positionalIndex):\n",
    "    for pos, term in enumerate(tokens_list):\n",
    "        \n",
    "        # If term exists in the positional index dictionary\n",
    "        if term in positionalIndex:   \n",
    "            \n",
    "            positionalIndex[term][0] = positionalIndex[term][0] + 1    # Increment total freq by 1\n",
    "            if fileno in positionalIndex[term][1]:                     # Check if the term has existed in that DocID before.\n",
    "                positionalIndex[term][1][fileno].append(pos)\n",
    "                \n",
    "            else:\n",
    "                positionalIndex[term][1][fileno] = [pos]\n",
    "        \n",
    "        # If term does not exist in the positional index dictionary  \n",
    "        else:\n",
    "            positionalIndex[term] = []                                 # Initialize the list. \n",
    "            positionalIndex[term].append(1)                            # The total frequency is 1. \n",
    "            positionalIndex[term].append({})                           # The postings list is initially empty. \n",
    "            positionalIndex[term][1][fileno] = [pos]                   # Add doc ID to postings list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"files\" => folder of text files \n",
    "file_names = [name for name in os.listdir(\"files\") if name.endswith('txt')] #list of text file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = {}        #positional index dictionary\n",
    "fileno = 0\n",
    "for file in file_names:\n",
    "    All_tokens   =  []            \n",
    "    tokens_temp  =  []\n",
    "    Lists = read_from_file(doc_name='files/'+file,lines=True,split=False,both=True)\n",
    "    for List in Lists:\n",
    "        for row in List:\n",
    "            token_temp = rtokenize(row)\n",
    "            All_tokens = All_tokens+token_temp\n",
    "    tokens_list = remove_stop_words(All_tokens)\n",
    "    tokens_list = Stemming(tokens_list)\n",
    "    positional_index(tokens_list,pos_index)\n",
    "    fileno += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ahm': [2, {0: [0], 1: [24]}],\n",
       " 'moham': [1, {0: [1]}],\n",
       " 'rana': [1, {0: [2]}],\n",
       " 'andrew': [2, {0: [3], 1: [26]}],\n",
       " 'ali': [2, {0: [4], 1: [25]}],\n",
       " 'menna': [1, {0: [5]}],\n",
       " 'gigi': [1, {0: [6]}],\n",
       " 'emma': [1, {0: [7]}],\n",
       " 'raad': [1, {0: [8]}],\n",
       " 'bebo': [1, {0: [9]}],\n",
       " 'lala': [1, {0: [10]}],\n",
       " 'boh': [1, {0: [11]}],\n",
       " 'dona': [1, {0: [12]}],\n",
       " 'nidaa': [1, {0: [13]}],\n",
       " 'play': [6, {1: [0, 4, 8, 12, 16, 20]}],\n",
       " '1': [1, {1: [1]}],\n",
       " 'boy': [6, {1: [2, 6, 10, 14, 18, 22]}],\n",
       " 'girl': [6, {1: [3, 7, 11, 15, 19, 23]}],\n",
       " '2': [1, {1: [5]}],\n",
       " '3': [1, {1: [9]}],\n",
       " '4': [1, {1: [13]}],\n",
       " '5': [1, {1: [17]}],\n",
       " '6': [1, {1: [21]}]}"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, {0: [0], 1: [24]}]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_index['ahm']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
