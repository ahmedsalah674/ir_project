{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cfc4dac7468c084d307aa08ac9296834a0ec3041907b428597ed5d5794ddbf16"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- imports---------\n",
    "import nltk #to import tokenize            \n",
    "import re   #to remove spicieal letters \n",
    "from nltk.corpus import stopwords #to remove stopwords\n",
    "from nltk.tokenize import TweetTokenizer #tokenize text and lowercacse it\n",
    "from os import path # to check if the file is there or not \n",
    "from collections import Counter #to return counter in dic {word:count}\n",
    "from nltk.stem import PorterStemmer \n",
    "# from nltk.tokenize import word_tokenize  \n",
    "# import string \n",
    "#    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer fun to return list of tokens in lowercase\n",
    "def rtokenize(text2):   \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(text2)\n",
    "    return tokens"
   ]
  },
  {
   "source": [
    "#test tokenizer fun\n",
    "# me_text=input(\"enter text \\n\")\n",
    "me_text_tokin1=rtokenize(doc_lines1[0])\n",
    "me_text_tokin2=rtokenize(doc_lines2[0])\n",
    "# print(me_text_tokins)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return set of words (terms)\n",
    "def set_tokins(tokins):\n",
    "    terms1=[]\n",
    "    for x in tokins:\n",
    "        if x not in terms1:\n",
    "            terms1.append(x)     \n",
    "    return terms1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set_tokins()\n",
    "set1=set_tokins(me_text_tokin1)\n",
    "set2=set_tokins(me_text_tokin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return counter for all items in word_1 list\n",
    "def get_count(word_l):\n",
    "    word_count_dict = {} \n",
    "    word_count_dict = Counter(word_l) \n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file in lines and return it in list line by line and if file doesn't exists return false \n",
    "# if split == false will return lines without any split \n",
    "# if split equal '\\t' or '\\n' will split lines as w need\n",
    "def read_from_file(doc_name,lines=True,split=False,both=False):\n",
    "        if isinstance(lines, bool) and isinstance(both ,bool):\n",
    "            if both and split:\n",
    "                return False \n",
    "            if path.exists(str(doc_name)):\n",
    "                file=open(str(doc_name),\"r\")\n",
    "                if not lines:\n",
    "                    doc_lines=[file.readline()]\n",
    "                    if split and (split == '\\t' or split == '\\n'):\n",
    "                        doc_lines=[word for word in doc_lines[0].split(split) if word !='']\n",
    "                    elif split:\n",
    "                        return False\n",
    "                else :\n",
    "                    doc_lines=file.readlines()\n",
    "                    if split and (split == '\\t' or split == '\\n'):\n",
    "                        doc_lines=[word for line in doc_lines for word in line.split(split) if word !='']\n",
    "                    elif split:\n",
    "                        return False\n",
    "                file.close()\n",
    "                if both:\n",
    "                   doc_lines=[word.split('\\t') for line in doc_lines for word in line.split('\\n') if word !='' ]  \n",
    "                return doc_lines\n",
    "            else: \n",
    "                return False;\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name1='new file test.txt'\n",
    "lis=read_from_file(doc_name=file_name1,lines=True,split=False,both=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['playing1', 'boys', 'girles'],\n",
       " ['playing2', 'boys', 'girles'],\n",
       " ['playing3', 'boys', 'girles'],\n",
       " ['playing4', 'boys', 'girles'],\n",
       " ['playing5', 'boys', 'girles'],\n",
       " ['playing6', 'boys', 'girles']]"
      ]
     },
     "metadata": {},
     "execution_count": 233
    }
   ],
   "source": [
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return indexes of one element \n",
    "def indexes(lis,val):\n",
    "    return[index for index, value in enumerate(lis) if value == val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 3, 11]\n"
     ]
    }
   ],
   "source": [
    "# test indexes fun (list ,value) and return the indexes of value \n",
    "lis=[1,2,5,2,3,86,37,3781,72,1,1,2]\n",
    "print(indexes(lis,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming fun (list_of_words) and return another list after stem it \n",
    "def Stemming(tokins):\n",
    "    stemmer = PorterStemmer() \n",
    "    reviews_stem = [] \n",
    "    reviews_stem = [stemmer.stem(word) for word in tokins]\n",
    "    return reviews_stem\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['playing1', 'boy', 'girl']"
      ]
     },
     "metadata": {},
     "execution_count": 234
    }
   ],
   "source": [
    "#test stemming fun (list_of_tokins)\n",
    "Stemming(lis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write func in file (file_name,list_contant,replase,Taps=False,Lines=False) return file have the name of file_name var and the contant list and if Taps = True return words in same line with \\t or Lines have  \n",
    "def write_file(file_name,lis,replace=False,Taps=False,Lines=False):\n",
    "    if replace:\n",
    "        file= open(file_name,\"w\")\n",
    "    else:\n",
    "        file= open(file_name,\"a\")\n",
    "    if Taps and not Lines :\n",
    "        file.write('\\t'.join(lis[0:]))\n",
    "        file.close()\n",
    "    elif Lines and not Taps :\n",
    "        file.write('\\n'.join(lis[0:]))\n",
    "        file.close()\n",
    "    else: \n",
    "        file.close()\n",
    "        return False\n",
    "    # else:\n",
    "    #     file= open(file_name,\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(lis=me_text_tokin1,file_name='new file test.txt',Taps=True,Lines=False,replace=False)"
   ]
  }
 ]
}