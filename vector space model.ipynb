{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ir.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "cfc4dac7468c084d307aa08ac9296834a0ec3041907b428597ed5d5794ddbf16"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re \n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_env(env_path):\n",
        "    with open(env_path, 'r', encoding=\"utf-8\") as env:\n",
        "            env_settings = env.read()\n",
        "            env_settings = re.sub(' ','',env_settings)\n",
        "            env_settings = env_settings.split('\\n')\n",
        "            env_settings = [setting.split('=') for setting in env_settings]\n",
        "            return dict(env_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def settings(env_settings):\n",
        "    language = env_settings['language']\n",
        "    path = env_settings['path']\n",
        "    type_files = env_settings['type_files']\n",
        "    number_files= int(env_settings['number_files'])\n",
        "    return language,path,type_files,number_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "language,path,type_files,number_files=settings(read_env('env.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fileNames(path,type_files):\n",
        "    file_names = [name for name in os.listdir(path) if name.endswith(type_files)]\n",
        "    file_dirs=[]\n",
        "    for name in file_names:\n",
        "        file_dirs.append(path + '/' +name)\n",
        "    return file_dirs,file_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Stemming(tokins):\n",
        "    stemmer = PorterStemmer() \n",
        "    reviews_stem = [] \n",
        "    reviews_stem = [stemmer.stem(word) for word in tokins]\n",
        "    return reviews_stem "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(query): # get text and return list of tokens withot stopwords and lowercase \n",
        "    stopword = stopwords.words(language) #get list of stop words in language\n",
        "    tokens = word_tokenize(query,language=language)  #get list of tokens\n",
        "    tokens_without_stop_word = [word.lower() for word in tokens if not word in stopword] # remove stopwords\n",
        "    return  tokens_without_stop_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_ls_steming_tokins():\n",
        "    inquery = input(\"Please enter a query:\\n\")\n",
        "    query_to = tokenize(inquery)\n",
        "    query =Stemming(query_to)\n",
        "    return query,query_to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        " file_dirs,file_names =fileNames(path,type_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tf_dict():\n",
        "    doc_vocab  = dict()\n",
        "   \n",
        "    for i in range(0,len(file_dirs)):\n",
        "        doc_vocab[i] = dict()\n",
        "        with open(file_dirs[i], 'r', encoding=\"utf-8\") as doc:\n",
        "            read_string = doc.read()  \n",
        "            tokens = tokenize(read_string)\n",
        "            tokens = Stemming(tokens)\n",
        "            # get dict of {doc_num : {word1 : word1_count, word2 : word2_count, .... }}\n",
        "            for words in tokens:\n",
        "                if words in doc_vocab[i]:\n",
        "                    doc_vocab[i][words] += 1\n",
        "                else:\n",
        "                    doc_vocab[i][words] = 1\n",
        "\n",
        "    term_pd = pd.DataFrame.from_dict(doc_vocab, orient='index')\n",
        "    term_pd.fillna(0,inplace=True)\n",
        "    term_pd.sort_index(inplace=True)\n",
        "    term_pd.to_csv('tf.csv')\n",
        "    return term_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "term_tf=tf_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ls=term_tf[term_tf > 0].count()\n",
        "idf=np.log(number_files/df_ls.values)\n",
        "idf_dict=pd.Series(data=idf,index=df_ls.index).to_dict()\n",
        "tf_idf=term_tf.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "for term in term_tf:\n",
        "    for doc in range(0,len(term_tf)):\n",
        "        if term_tf[term][doc] == 0:\n",
        "            tf_idf[term][doc] = 0\n",
        "        else:\n",
        "            tf_idf[term][doc] = (1 + np.log(term_tf[term][doc])) * np.log10(number_files/df_ls[term])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_idf.to_csv('tf_idf2.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "lenth_docs=[math.sqrt(sum((tf_idf**2).loc[i]) ) for i in tf_idf.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalize_data=tf_idf.copy()\n",
        "for term in tf_idf:\n",
        "    for doc in tf_idf.index:\n",
        "        if lenth_docs[doc]==0:\n",
        "            normalize_data[term][doc]=0\n",
        "        else:\n",
        "            normalize_data[term][doc]=tf_idf[term][doc] / lenth_docs[doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "query,query_to=get_ls_steming_tokins()\n",
        "tf_query=Counter(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_idf_query=dict()\r\n",
        "for term in tf_query:\r\n",
        "    if term in idf_dict:\r\n",
        "        tf_idf_query[term]=(1 + np.log(tf_query[term])) * idf_dict[term]\r\n",
        "    else:\r\n",
        "        tf_idf_query[term]= 0\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "lenth_query=0\r\n",
        "for i in tf_idf_query.values():\r\n",
        "    lenth_query=lenth_query+(i**2)\r\n",
        "lenth_query=math.sqrt(lenth_query)\r\n",
        "\r\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalize_query=tf_idf_query.copy()\n",
        "for term in tf_idf_query:\n",
        "    if lenth_query !=0:\n",
        "        normalize_query[term]=  tf_idf_query[term] / lenth_query\n",
        "    else:\n",
        "        normalize_query[term]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim=normalize_data.copy()\n",
        "for term in normalize_data:\n",
        "    if term not in normalize_query:\n",
        "        sim.drop(term,inplace=True,axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "for term in normalize_query:\r\n",
        "    for doc in normalize_data.index:\r\n",
        "        sim[term][doc]=normalize_data[term][doc] * normalize_query[term]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sim of doc1.txt -> 0.45\nsim of doc10.txt -> 0.29\nsim of doc2.txt -> 0.03\nsim of doc3.txt -> 0.00\nsim of doc4.txt -> 0.00\nsim of doc5.txt -> 0.00\nsim of doc6.txt -> 0.29\nsim of doc7.txt -> 0.03\nsim of doc8.txt -> 0.29\nsim of doc9.txt -> 0.03\n"
          ]
        }
      ],
      "source": [
        "for i in range(0,len(sim.index)):\n",
        "    print('sim of',file_names[i], '->', \"%.2f\" % sum(sim.loc[i]))"
      ]
    }
  ]
}